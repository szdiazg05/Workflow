{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22071ca3",
   "metadata": {},
   "source": [
    "# AI Agent\n",
    "\n",
    "Desarrollado por: Sofia Diaz \n",
    "\n",
    "# Descripcion del Proyecto\n",
    "\n",
    "Este agente AI implementa un workflow con:\n",
    "- Routing: Seleccion inteligente de fuente de datos\n",
    "- Retrieval: Extraccion de informacion contextual\n",
    "- Multimodalidad: Soporte para CSV, TXT e imagenes\n",
    "- Structured Outputs: Formato JSON con Pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e706ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key cargada correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\szdia\\Documents\\IA_Agent\\Entorno\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\szdia\\Documents\\IA_Agent\\src\\retriever.py:4: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "# Modulos del agente\n",
    "\n",
    "from src.config import GOOGLE_API_KEY\n",
    "from src.router import router\n",
    "from src.retriever import retriever\n",
    "from src.stylist import stylist\n",
    "from src.workflow import run_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe44229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo seleccionado: datos_clima_mexico.csv\n",
      "\n",
      "Archivo seleccionado: GPT-41_PromptingGuide.txt\n",
      "\n",
      "Archivo seleccionado: maiz_info.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PASO 1: Seleccion de Archivo\n",
    "\n",
    "test_questions = [\n",
    "    \"¬øCu√°l fue la temperatura m√°xima en agosto 2021?\",\n",
    "    \"¬øQu√© dice sobre el uso de XML tags?\",\n",
    "    \"¬øCu√°ntas razas de ma√≠z son nativas?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    archivo = router(q)\n",
    "    print(f\"Archivo seleccionado: {archivo}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066efc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: La temperatura media nacional en enero de 1985 fue de 15.9.\n",
      "Respuesta: Para utilizar plenamente las capacidades de agente de GPT-4.1, el contexto recomienda incluir tres tipos de recordatorios clave en los **system prompts**:\n",
      "\n",
      "1.  **Persistencia:** Asegura que el modelo entienda que est√° en un turno de m√∫ltiples mensajes y evita que ceda el control al usuario prematura...\n",
      "Respuesta: Esta informacion no esta disponible en la imagen.\n"
     ]
    }
   ],
   "source": [
    "# PASO 2: Respuesta con Contexto\n",
    "\n",
    "respuesta_csv = retriever(\n",
    "    \"¬øCu√°l fue la temperatura promedio nacional en enero de 1985?\",\n",
    "    \"datos_clima_mexico.csv\"\n",
    ")\n",
    "print(f\"Respuesta: {respuesta_csv}\")\n",
    "\n",
    "respuesta_txt = retriever(\n",
    "    \"¬øQu√© dice sobre system prompts en agentes?\",\n",
    "    \"GPT-41_PromptingGuide.txt\"\n",
    ")\n",
    "print(f\"Respuesta: {respuesta_txt[:300]}...\")\n",
    "\n",
    "respuesta_imagen = retriever(\n",
    "    \"¬øPor qu√© el huitlacoche es superior en nutrientes al ma√≠z tradicional?\",\n",
    "    \"maiz_info.jpg\"\n",
    ")\n",
    "print(f\"Respuesta: {respuesta_imagen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d622f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En M√©xico existen 59 razas de ma√≠z nativas que se han reunido en 7 grupos principales. Estas razas son el resultado de 8 mil a√±os de domesticaci√≥n.\n",
      "M√©xico cuenta con 59 razas de ma√≠z nativas clasificadas en 7 grupos. Son el fruto de 8,000 a√±os de domesticaci√≥n.\n",
      "¬°M√©xico es el para√≠so del elote! üåΩ Tenemos 59 razas nativas en 7 grupos, perfeccionadas tras 8,000 a√±os de puro amor agr√≠cola. ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# PASO 3: Formato en 3 Tonos\n",
    "respuesta_ejemplo = \"En M√©xico existen 59 razas de ma√≠z nativas que se han reunido en 7 grupos principales. Estas razas son el resultado de 8 mil a√±os de domesticaci√≥n.\"\n",
    "\n",
    "resultado = stylist(respuesta_ejemplo)\n",
    "\n",
    "print(resultado['Respuesta_original'])\n",
    "print(resultado['Respuesta_directa'])\n",
    "print(resultado['Respuesta_divertida'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcefca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo usado: GPT-41_PromptingGuide.txt\n",
      "Respuesta Original:\n",
      "Esta informaci√≥n no est√° disponible en los datos proporcionados.\n",
      "Respuesta Concisa:\n",
      "No se dispone de la informaci√≥n solicitada en los datos suministrados.\n",
      "Respuesta Graciosa:\n",
      "¬°Vaya! Mis archivos est√°n m√°s vac√≠os que una nevera un lunes por la ma√±ana. üßä Esa info no aparece por ning√∫n lado. üïµÔ∏è‚Äç‚ôÇÔ∏è\n"
     ]
    }
   ],
   "source": [
    "# Workflow completo\n",
    "\n",
    "pregunta_demo = \"¬øQu√© dice el cookbook sobre el uso de etiquetas XML para GPT-4.1?\"\n",
    "\n",
    "resultado_final = run_workflow(pregunta_demo)\n",
    "\n",
    "print(f\"Archivo usado: {resultado_final['selected_file']}\")\n",
    "print(f\"Respuesta Original:\\n{resultado_final['Respuesta_original'][:300]}\")\n",
    "print(f\"Respuesta Concisa:\\n{resultado_final['Respuesta_directa']}\")\n",
    "print(f\"Respuesta Graciosa:\\n{resultado_final['Respuesta_divertida']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f2fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: datos_clima_mexico.csv\n",
      "Original:\n",
      "Esta informaci√≥n no est√° disponible en los datos proporcionados.\n",
      "Conciso:\n",
      "No se encuentra la informaci√≥n solicitada en los datos proporcionados.\n",
      "Gracioso:\n",
      "¬°Ups! Mis datos est√°n m√°s vac√≠os que mi nevera un lunes. üïµÔ∏è‚Äç‚ôÇÔ∏è No encontr√© nada por aqu√≠. üö´\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "\n",
    "\"\"\"Los siguientes son los 6 test cases en el README para validar el funcionamiento.\"\"\"\n",
    "\n",
    "test_cases = [\n",
    "    \"¬øQu√© dice el cookbook sobre el uso de etiquetas XML para GPT-4.1?\",\n",
    "    \"¬øAlg√∫n consejo para promptear a GPT-5?\", \n",
    "    \"¬øTop 5 estados m√°s calientes en agosto de 2021?\",\n",
    "    \"¬øCu√°les fueron las 3 temperaturas m√≠nimas en diciembre de 2025?\",\n",
    "    \"¬øCu√°ntas razas de ma√≠z son nativas en M√©xico?\",\n",
    "    \"¬øPor qu√© el huitlacoche es superior en nutrientes al ma√≠z tradicional?\"\n",
    "]\n",
    "pregunta_test = test_cases[0:5] \n",
    "resultado = run_workflow(pregunta_test)\n",
    "\n",
    "print(f\"Archivo: {resultado['selected_file']}\")\n",
    "print(f\"Original:\\n{resultado['Respuesta_original']}\")\n",
    "print(f\"Conciso:\\n{resultado['Respuesta_directa']}\")\n",
    "print(f\"Gracioso:\\n{resultado['Respuesta_divertida']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c98125",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reflexion final\n",
    "\n",
    "Utilice LangChain como framework principal a excepcion del Procesamiento de im√°genes, ya que usa SDK directa de Google porque el soporte multimodal de LangChain est√° en desarrollo.\n",
    "\n",
    "# Tokens y Costo\n",
    "\n",
    "Para la arquitectura de este proyecto, me enfoque principalmente en lograr que funcionara y en entender cada paso del proceso; y por lo tanto, siento que la arquitectura si puede escalar, porque divide el problema en partes peque√±as. Esto ayuda a que las respuestas sean m√°s claras y controladas.\n",
    "\n",
    "Sin embargo, tambi√©n noto que cada pregunta del usuario implica varias llamadas al modelo, y eso aumenta el uso de tokens y los costos si se utilizara en un sistema real con muchos usuarios.\n",
    "\n",
    "Creo que se podr√≠a optimizar reduciendo la cantidad de llamadas al modelo, usando modelos m√°s peque√±os para tareas simples como el enrutamiento, o procesando mejor los datos antes de enviarlos al modelo.\n",
    "\n",
    "Consumo actual: 3.500 - 5.500 tokens por request que tiene un costo aproximado de 0.03 USD (Teniendo en cuenta que el modelo cueste aprox. 3 USD por 1M de tokens de entrada y 10 USD por 1M de tokens de salida) y podria optimizarse usando modelos economicos para tareas simples o reducir el numero de llamadas al modelo para reducir el numeor de tokens.\n",
    "\n",
    "# Latencia\n",
    "\n",
    "Considero que si podria ser un poco mas lento para usar en tiempo real, porque aqui se hacen varios pasos. Aun asi, entiendo que esa division tambien hace que el sistema sea mas ordenado y confiable, lo que genera un equilibrio entre rapidez y precision.\n",
    "\n",
    "Para hacerlo mas rapido, se podrian usar modelos m√°s rapidos para algunas partes del proceso o simplificar algunos pasos del flujo.\n",
    "\n",
    "# Evaluacion\n",
    "\n",
    "Una forma mas confiable seria probar el sistema con preguntas especificas y comparar si realmente usa el archivo correcto y si la respuesta coincide con la informacion disponible.\n",
    "\n",
    "Tambi√©n se evaluo si el sistema evita responder cuando la informacion no esta en los datos, con las preguntas trampa del ejercicio. Eso me parecio interesante porque obliga a que el agente no invente cosas.\n",
    "\n",
    "Como es un flujo paso a paso, cada parte se podr√≠a revisar por separado e identificar falencias o puntos a mejorar. \n",
    "\n",
    "# Autonom√≠a\n",
    "\n",
    "Desde lo que hasta ahora se, un agente tipo ReAct tendr√≠a m√°s libertad para decidir qu√© hacer en cada momento, en lugar de seguir un flujo fijo como el que construi. En ese sentido, eso podria ser bueno porque seria mas flexible para preguntas diferentes o inesperadas. Pero tambien podria ser mas dificil de controlar, revisar si algo sale mal, y tal vez use m√°s tokens.\n",
    "\n",
    "Para este ejercicio, siento que el flujo paso a paso fue una buena decision porque hace que todo sea mas claro y mas facil de entender.\n",
    "\n",
    "# Aprendizajes Clave\n",
    "\n",
    "Mi mayor aprendizaje fue darme cuenta de que un sistema de IA no es solo hacer una pregunta y obtener una respuesta, pude entender como cada parte del proceso tenia un proposito y el poder dise√±ar un flujo logico que ayude a que el modelo use la informacion correcta fue un reto. \n",
    "\n",
    "Aunque en varios momentos tuve que investigar y apoyarme en herramientas para avanzar, siento que eso hace parte del proceso de aprendizaje y este ejercicio me dejo muy motivada a seguir mejorando. Si me veo trabajando en este tipo de soluciones y desarrollando m√°s habilidades con la pr√°ctica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
